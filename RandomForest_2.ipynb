{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shafwatm/DS340/blob/main/RandomForest_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f9a151d-6246-4572-a8c6-d52385fb43b6",
      "metadata": {
        "id": "3f9a151d-6246-4572-a8c6-d52385fb43b6"
      },
      "source": [
        "## Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca6515d5-b4d4-410c-ad48-4ae7df43cb1f",
      "metadata": {
        "id": "ca6515d5-b4d4-410c-ad48-4ae7df43cb1f"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from pyspark.sql import Row\n",
        "import plotly.express as px\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import col, column\n",
        "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, FloatType\n",
        "from pyspark.sql.functions import col, column, when, countDistinct\n",
        "from pyspark.sql.functions import expr\n",
        "from pyspark.sql.functions import split\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, IndexToString\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
        "from decision_tree_plot.decision_tree_parser import decision_tree_parse\n",
        "from decision_tree_plot.decision_tree_plot import plot_trees\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d87d0e09-3ca4-4fc9-ba19-8f72a787c47f",
      "metadata": {
        "id": "d87d0e09-3ca4-4fc9-ba19-8f72a787c47f"
      },
      "outputs": [],
      "source": [
        "crime=SparkSession.builder.master(\"local\").appName(\"CrimeDataAnalysis\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34ae1d72-f2ae-4efe-ae73-e7353a1738a4",
      "metadata": {
        "id": "34ae1d72-f2ae-4efe-ae73-e7353a1738a4"
      },
      "outputs": [],
      "source": [
        "crime.sparkContext.setCheckpointDir(\"~/scratch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50930a73-baa1-4e72-84fb-c09205795207",
      "metadata": {
        "id": "50930a73-baa1-4e72-84fb-c09205795207"
      },
      "source": [
        "## Uploading 2019-2023 Crime Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13061c62-e279-43ee-bdc8-4b3c6d98a991",
      "metadata": {
        "id": "13061c62-e279-43ee-bdc8-4b3c6d98a991"
      },
      "outputs": [],
      "source": [
        "Data19 = crime.read.csv(\"./Crimes_-_2019_20231112.csv\", header=True, inferSchema=True)\n",
        "Data20 = crime.read.csv(\"./Crimes_-_2020_20231112.csv\", header=True, inferSchema=True)\n",
        "Data21 = crime.read.csv(\"./Crimes_-_2021_20231112.csv\", header=True, inferSchema=True)\n",
        "Data22 = crime.read.csv(\"./Crimes_-_2022_20231016.csv\", header=True, inferSchema=True)\n",
        "Data23 = crime.read.csv(\"./Crimes_-_2023_20231016.csv\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dda5c50c-0a1c-47ff-a157-aadbc240993c",
      "metadata": {
        "id": "dda5c50c-0a1c-47ff-a157-aadbc240993c"
      },
      "source": [
        "## Data Merging with 2022 - 2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa2026f3-388f-4ac8-b3aa-60593db1a01a",
      "metadata": {
        "id": "fa2026f3-388f-4ac8-b3aa-60593db1a01a"
      },
      "outputs": [],
      "source": [
        "Data19_23 = Data19.union(Data20).union(Data21).union(Data22).union(Data23)\n",
        "Data19_23.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a785e41-710f-44b4-806d-290c491fcdfb",
      "metadata": {
        "id": "2a785e41-710f-44b4-806d-290c491fcdfb"
      },
      "outputs": [],
      "source": [
        "print(f\"Total Entries in 2019 to 2023: {Data19_23.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "613e01ff-bad9-4aa0-9bdc-84f0de4e7de7",
      "metadata": {
        "id": "613e01ff-bad9-4aa0-9bdc-84f0de4e7de7"
      },
      "source": [
        "## Removing rows with null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a283d93-5efa-4c66-9b54-7ed71cef050f",
      "metadata": {
        "id": "2a283d93-5efa-4c66-9b54-7ed71cef050f"
      },
      "outputs": [],
      "source": [
        "df = Data19_23.select(\"Date\", \"Block\", \"Primary Type\", \"Description\", \"Location Description\", \"Arrest\", \"Domestic\",\n",
        "                     \"Beat\", \"District\", \"Ward\", \"Community Area\", \"Year\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "610cd623-ee47-4ec6-8f9b-9e7906a480c4",
      "metadata": {
        "id": "610cd623-ee47-4ec6-8f9b-9e7906a480c4"
      },
      "outputs": [],
      "source": [
        "df_clean = df.dropna(how = 'any')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a40ea54c-0475-4e1c-b8d1-3c0f16991bb3",
      "metadata": {
        "id": "a40ea54c-0475-4e1c-b8d1-3c0f16991bb3"
      },
      "outputs": [],
      "source": [
        "print(f\"Total Entries after cleaning in 2019 to 2023: {df_clean.count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e34db1ea-2498-4e6a-871c-9896ed8c5ac7",
      "metadata": {
        "id": "e34db1ea-2498-4e6a-871c-9896ed8c5ac7"
      },
      "outputs": [],
      "source": [
        "df_clean.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74af7d6b-1236-42e3-a84e-33e69cf18e17",
      "metadata": {
        "id": "74af7d6b-1236-42e3-a84e-33e69cf18e17"
      },
      "outputs": [],
      "source": [
        "# Display the first two rows of the dataset\n",
        "first_two_rows = df_clean.take(2)\n",
        "\n",
        "# Print each row with column names for clarity\n",
        "for row in first_two_rows:\n",
        "    for col_name in df_clean.columns:\n",
        "        print(f\"{col_name}: {row[col_name]}\")\n",
        "    print(\"\\n---\\n\")  # Separator between rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d425dd94-42f0-4ec2-bd42-72d1afe3e47a",
      "metadata": {
        "id": "d425dd94-42f0-4ec2-bd42-72d1afe3e47a"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import countDistinct\n",
        "\n",
        "# List of columns to check for cardinality\n",
        "columns_to_check = [\"Primary Type\", \"Description\", \"Location Description\", \"Ward\", \"Year\"]\n",
        "\n",
        "# Query to count distinct values in each column\n",
        "for column in columns_to_check:\n",
        "    distinct_count = df_clean.select(countDistinct(col(column)).alias(column)).collect()[0][column]\n",
        "    print(f\"Distinct count in {column}: {distinct_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba1adb81-e44b-48f5-bc58-3e2b540ff01a",
      "metadata": {
        "id": "ba1adb81-e44b-48f5-bc58-3e2b540ff01a"
      },
      "source": [
        "## Map-Reduce for data analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "268a7112-e224-4f35-b67c-b640db63d5db",
      "metadata": {
        "id": "268a7112-e224-4f35-b67c-b640db63d5db"
      },
      "outputs": [],
      "source": [
        "mapped_primary_type = df_clean.rdd.map(lambda row: (row[\"Primary Type\"], 1))\n",
        "reduced_primary_type = mapped_primary_type.reduceByKey(lambda a, b: a + b)\n",
        "sorted_primary_type = reduced_primary_type.sortBy(lambda x: x[1], ascending=False)\n",
        "primary_type_counts_sorted = sorted_primary_type.collect()\n",
        "\n",
        "primary = [] #empty list for visualization\n",
        "counts = [] #empty list for visualization\n",
        "\n",
        "for primary_type, count in primary_type_counts_sorted:\n",
        "    print(f\"{primary_type}: {count}\")\n",
        "    primary.append(primary_type)  #the first column (primary type)\n",
        "    counts.append(int(count))  #the second column (counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45739a41-1b74-49ed-9508-3ff07b2d388a",
      "metadata": {
        "id": "45739a41-1b74-49ed-9508-3ff07b2d388a"
      },
      "source": [
        "## Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec822138-4c7e-4ee6-9da2-48e7fef3a610",
      "metadata": {
        "id": "ec822138-4c7e-4ee6-9da2-48e7fef3a610"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "# Cardinality reduction for 'Description' column\n",
        "desc_ct = df_clean.groupBy(\"Description\").count()\n",
        "desc_ct_sorted = desc_ct.sort(desc_ct['count'], ascending=False)\n",
        "desc_top_32 = desc_ct_sorted.limit(32)\n",
        "desc_list = desc_top_32.select(\"Description\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "# Update 'Description' column to keep only top 32 categories\n",
        "df_clean = df_clean.withColumn(\"Description\", when(col(\"Description\").isin(desc_list), col(\"Description\")).otherwise(\"Other\"))\n",
        "\n",
        "# Cardinality reduction for 'Location Description' column\n",
        "loc_ct = df_clean.groupBy(\"Location Description\").count()\n",
        "loc_ct_sorted = loc_ct.sort(loc_ct['count'], ascending=False)\n",
        "loc_top_32 = loc_ct_sorted.limit(32)\n",
        "loc_list = loc_top_32.select(\"Location Description\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "filtered_df = df_clean.filter(col(\"Description\").isin(desc_list) & col(\"Location Description\").isin(loc_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46b414ef-976e-4eff-ab0e-a364f8c64e3a",
      "metadata": {
        "id": "46b414ef-976e-4eff-ab0e-a364f8c64e3a"
      },
      "source": [
        "#### Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fff58dd-bd83-4811-bc2b-28c1f109c4c8",
      "metadata": {
        "id": "0fff58dd-bd83-4811-bc2b-28c1f109c4c8"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "\n",
        "df_model = filtered_df.select(\"Primary Type\", \"Description\", \"Location Description\", \"Arrest\", \"Domestic\", \"Year\")\n",
        "df_model = df_model.withColumn(\"Arrest\", when(col(\"Arrest\") == \"true\", 1).otherwise(0))\n",
        "df_model = df_model.withColumn(\"Domestic\", when(col(\"Domestic\") == \"true\", 1).otherwise(0))\n",
        "\n",
        "# Indexing and assembling\n",
        "inputs = [\"Primary Type\", \"Description\", \"Location Description\"]\n",
        "outputs = [input_col + \"_index\" for input_col in inputs]\n",
        "indexers = [StringIndexer(inputCol=input_col, outputCol=output_col).fit(df_model) for input_col, output_col in zip(inputs, outputs)]\n",
        "assembler = VectorAssembler(inputCols=[indexer.getOutputCol() for indexer in indexers] + [\"Domestic\"], outputCol=\"features\")\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf = RandomForestClassifier(labelCol=\"Arrest\", featuresCol=\"features\")\n",
        "# Pipeline\n",
        "pipeline = Pipeline(stages=indexers + [assembler, rf])\n",
        "\n",
        "# Splitting data\n",
        "train_data, validation_data, test_data = df_model.randomSplit([0.60, 0.20, 0.20], seed=17)\n",
        "\n",
        "train_data.cache()\n",
        "test_data.cache()\n",
        "\n",
        "# Parameter grid for model tuning\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(rf.numTrees, [10, 15]) \\\n",
        "    .addGrid(rf.maxDepth, [5, 7]) \\\n",
        "    .addGrid(rf.maxBins, [32, 40]) \\\n",
        "    .build()\n",
        "\n",
        "# Evaluator\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"Arrest\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "# Cross-validation\n",
        "crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=3)\n",
        "\n",
        "# Fit model using training data\n",
        "cvModel = crossval.fit(train_data)\n",
        "\n",
        "# Use validation data for hyperparameter tuning\n",
        "validation_predictions = cvModel.transform(validation_data)\n",
        "\n",
        "# Select the best model\n",
        "best_model = cvModel.bestModel\n",
        "\n",
        "# Extract best hyperparameters\n",
        "best_numTrees = best_model.stages[-1]._java_obj.getNumTrees()\n",
        "best_maxDepth = best_model.stages[-1]._java_obj.getMaxDepth()\n",
        "best_maxBins = best_model.stages[-1]._java_obj.getMaxBins()\n",
        "\n",
        "# Evaluate the best model on validation data\n",
        "validation_accuracy = evaluator.evaluate(validation_predictions)\n",
        "\n",
        "# Create a DataFrame for hyperparameters and their evaluation metrics\n",
        "import pandas as pd\n",
        "hyperparams_eval_df = pd.DataFrame({\n",
        "    \"numTrees\": [best_numTrees],\n",
        "    \"maxDepth\": [best_maxDepth],\n",
        "    \"maxBins\": [best_maxBins],\n",
        "    \"validation_accuracy\": [validation_accuracy]\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "output_path = \"./RF_Hyperparameters_Evaluation.csv\"\n",
        "hyperparams_eval_df.to_csv(output_path, index=False)\n",
        "\n",
        "# Use the best model to make predictions on the test data\n",
        "predictions = best_model.transform(test_data)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test Accuracy = %g\" % accuracy)\n",
        "\n",
        "train_data.unpersist()\n",
        "test_data.unpersist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2038ebf-29a6-4d34-b86a-7c7c4de47bac",
      "metadata": {
        "id": "f2038ebf-29a6-4d34-b86a-7c7c4de47bac"
      },
      "source": [
        "#### Model Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffcc9d08-99ff-49e5-941e-3f978d1f6883",
      "metadata": {
        "id": "ffcc9d08-99ff-49e5-941e-3f978d1f6883"
      },
      "outputs": [],
      "source": [
        "best_rf_model = cvModel.bestModel.stages[-1]\n",
        "\n",
        "importances = best_rf_model.featureImportances\n",
        "\n",
        "\n",
        "feature_names = [indexer.getOutputCol() for indexer in indexers] + [\"Domestic\"]\n",
        "\n",
        "\n",
        "importances_with_names = [(feature_names[i], importance) for i, importance in enumerate(importances)]\n",
        "\n",
        "\n",
        "sorted_importances = sorted(importances_with_names, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "names, values = zip(*sorted_importances)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(names, values)\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.ylabel('Features')\n",
        "plt.title('Feature Importances in Random Forest Model')\n",
        "plt.gca().invert_yaxis()  # To display the highest importance at the top\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ca26dac-6271-433e-a1a9-40d20fb36890",
      "metadata": {
        "id": "9ca26dac-6271-433e-a1a9-40d20fb36890"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Precision\n",
        "precision_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Arrest\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "precision = precision_evaluator.evaluate(predictions)\n",
        "print(\"Precision = %g\" % precision)\n",
        "\n",
        "# Recall\n",
        "recall_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Arrest\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "recall = recall_evaluator.evaluate(predictions)\n",
        "print(\"Recall = %g\" % recall)\n",
        "\n",
        "# F1 Score\n",
        "f1_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"Arrest\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "f1_score = f1_evaluator.evaluate(predictions)\n",
        "print(\"F1 Score = %g\" % f1_score)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python\n(ds410_f23)",
      "language": "python",
      "name": "ds410_f23"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}